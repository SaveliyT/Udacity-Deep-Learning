{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n",
      "27\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "print (vocabulary_size)\n",
    "print (first_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_size = vocabulary_size * vocabulary_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n",
      "[ 1 13  1  9  5  0  7  7  1  0 11 19 19  5 26  0  0  8  7  0 14  3 20  3 21\n",
      " 15 20  0 20 15 19  7 18  5 18  8  1  0 19 12  1  0  0  5  8 15 14  9  0 18\n",
      "  5 19  1 15  9 20  9 23  5  3  5  0  0 22]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print((train_batches.next()[0].argmax(axis=1)))\n",
    "\n",
    "b = valid_batches.next()\n",
    "print (len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input :  (64, 27)\n",
      "input_gate (64, 64)\n",
      "forget_gate (64, 64)\n",
      "update (64, 64)\n",
      "state (64, 64)\n",
      "output_gate (64, 64)\n",
      "input_gate (1, 64)\n",
      "forget_gate (1, 64)\n",
      "update (1, 64)\n",
      "state (1, 64)\n",
      "output_gate (1, 64)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    \n",
    "    print ('input_gate', input_gate.shape)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    print ('forget_gate', forget_gate.shape)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    print ('update', update.shape)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    print ('state', state.shape)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    print ('output_gate', output_gate.shape)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "#   print (train_inputs[0].shape)\n",
    "  print (len(train_data))\n",
    "  for i in train_inputs:\n",
    "    print('input : ', i.shape)\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298439 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "jrqurlzeeonix  ezkeea ptisticyemyrado m fuia uloveecncz u mlzsg o eduzqt vsljxee\n",
      "k pltolu kvs ezv w xh  f ktibgol siendbb  tbvryy rmawbiq etbdvclufpfpips saibe u\n",
      "mmi iny ye uxb  rj tjr hl lyoc p esafnnekratsnic ym  vx b n edboevood ec rnxmktm\n",
      "mfceorpvmh peeddcb qendfy a nlrmdnfbnqeyrrcnanfmibntjvil x sneobnsb denipha atce\n",
      "g nfro  a ontsadjaydbc dasnrrianntxys coeqa iue ew wtotu  o nmea teggbweoysg goo\n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 100: 2.588071 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.10\n",
      "Validation set perplexity: 11.00\n",
      "Average loss at step 200: 2.257939 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.79\n",
      "Validation set perplexity: 9.25\n",
      "Average loss at step 300: 2.102918 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 2.005428 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 500: 1.943619 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 600: 1.916431 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 700: 1.863610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 800: 1.823864 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 900: 1.833307 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1000: 1.825855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "k one five londeg to elores foundisu knither giling one wis feminkmoloduratan cl\n",
      "tacies has leviane thear kend welenciishom ristrish freechip ropersily one fiver\n",
      "wicaltible main migrem intomess generity in fay yey from six feer one eiver refi\n",
      "zer in whowy pans syviculy the kige five thoust by the un had expericnite landin\n",
      "lined fown molic and hindip juhemal reade dissed gillucing phecitigy snanrationa\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.775557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1200: 1.754565 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1300: 1.730212 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1400: 1.741645 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1500: 1.737720 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1600: 1.745591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1700: 1.708260 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.672965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 1900: 1.646636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2000: 1.694219 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "kial rett of the over ollitt of associated worther is ariestent the comedy that \n",
      "reple wis randonso turrep one three andleces as not irssmias this one eight diis\n",
      "y someuthest their waidey latflinely indinifin consinging propenslinesionetwy us\n",
      "nors the inceptifiand in special has tenttrish birther english branothemsition a\n",
      "d arican concurfers of codnm one hageparth have maharand xach lay chried argence\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2100: 1.684428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2200: 1.675329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2300: 1.641120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2400: 1.655196 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2500: 1.678187 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2600: 1.656173 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2700: 1.652759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2800: 1.646428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2900: 1.647960 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3000: 1.646013 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "ver around lack through intheroct world wh hench as them one nine seven one five\n",
      "x emparay death but they the have counch in the edplates lasting mechnovelans of\n",
      "fitalls that schuil one nine six the play to eight bleame convedied retwole they\n",
      "van in one nine s in to inmernations by care dectempe artlsider old formed tomat\n",
      "with with lavilate in earli tandard the two zero one one six parther cound d for\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3100: 1.630749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3200: 1.642616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3300: 1.635146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3400: 1.666730 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3500: 1.655666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3600: 1.667017 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3700: 1.644082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3800: 1.642687 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3900: 1.638123 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4000: 1.653262 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "loting to be also as iti is with intros temperpe to forced that of mose froming \n",
      "ro had ppo likna and theor rit wipl lebant lawnent hapis of also last blan those\n",
      "zer on samilard politic council rustera trud the constrascison by defortewue can\n",
      "ja wir one seven icruens literate platamiy or florth diblands aslambiese complon\n",
      "matamete this relahary de into title is hands also fiem britable to be verthoibe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4100: 1.636829 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.634402 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4300: 1.615445 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4400: 1.608174 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4500: 1.613508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4600: 1.614967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4700: 1.627083 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.626739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4900: 1.632958 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.601747 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "mentrotorath records he smelians itsillipicals of plocativies is itworserseven s\n",
      "quite in computer bluate dydals more past engire one nine five nine julzeary wna\n",
      "fuarestall nations by isl remia risessia destronse year exterpored but time to b\n",
      "qued of one mish and the untervition arreature two five metailorman s ross in of\n",
      "been one kinks that s dieving manuzer science pews have desporoy to brited sanza\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5100: 1.604250 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5200: 1.588251 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5300: 1.584744 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5400: 1.580914 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5500: 1.567914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5600: 1.584275 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5700: 1.569054 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5800: 1.582727 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5900: 1.570534 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6000: 1.548835 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "them net on towh magize if al ever folor any connection court of it ortho ruth t\n",
      "us in the unites in octoners dubrachs the not to a jeing at through stantary int\n",
      "is the year jokell are incurd in metholops phication of the for three war since \n",
      "groundody collusanicility or sands cammed in pubte one four physicually ecoldity\n",
      "videmelize jelaw forch the localia single constitute s aldies the dytand to both\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6100: 1.565359 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6200: 1.537347 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6300: 1.542241 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6400: 1.539810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.555552 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.599090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.580411 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6800: 1.604514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6900: 1.580329 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 7000: 1.574294 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "det helded tonlown maneaze as p one nine eastic well a philimininch of that armi\n",
      "fter regecre intake actions to one nine four seven discover musul rind hat dayma\n",
      "mist numbers servance obarology and in as programe cherect vigian into leotus lu\n",
      "ground death gbeeking bkchevil  inficle in castal ous in the boundone affect the\n",
      " coundmenced and sisting auriad on according of one four three zero zero havoy c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run( \\\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print( \\\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float( \\\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    \n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp( \\\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Biases\n",
    "\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    big_i_matrix = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    big_o_matrix = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "#         input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#         forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#         update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    \n",
    "        out_1 = tf.matmul(i, big_i_matrix)\n",
    "        out_2 = tf.matmul(o, big_o_matrix)\n",
    "        \n",
    "\n",
    "        input_matr_1, input_matr_2, input_matr_3, input_matr_4 = tf.split(out_1, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        output_matr_1, output_matr_2, output_matr_3, output_matr_4 = tf.split(out_2, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        input_gate = tf.sigmoid(input_matr_1 + output_matr_1 + ib)\n",
    "        forget_gate = tf.sigmoid(input_matr_2 + output_matr_2 + fb)\n",
    "        update = input_matr_3 + output_matr_3 + cb\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(input_matr_4 + output_matr_4 + ob)\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append( \\\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    #   print (train_inputs[0].shape)\n",
    "    #   print (output.shape)\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                        saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                    labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group( \\\n",
    "            saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell( \\\n",
    "            sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297665 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "w pysvp eqgpmnimsyjo  winzx nyiv vd  ahiabsdjiuqcese mzil twosr peefe eh ga  y o\n",
      "\n",
      "t w bar hy  d ep  asrbrief lyeryii i amwl ldarij rsbwnektxurebjlwdsezudfu rrkwd \n",
      "\n",
      "izenx ea lzzwralnbopyynebhvkcmld ixeiagnrb f ntmjj omltsqnuexeeer p  o fyzerv jx\n",
      "\n",
      "eczmovz aqoyrryvjiua evwt dssoexlnatne exqp kdzpwborwna  ovafpodo gn mp tlehmghv\n",
      "\n",
      "dbdrsif toepdulwminetqtcbns nwdwtsewnze ndnxsnihp unljhik dbkmqdvnlqeseyqxbot qs\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.581770 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 11.51\n",
      "Average loss at step 200: 2.237587 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.24\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 300: 2.077978 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400: 2.028871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 500: 1.975203 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600: 1.897023 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 700: 1.871431 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 800: 1.864135 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 900: 1.842321 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1000: 1.842710 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "================================================================================\n",
      "ctic d ha out capetrant mukate sugran englion andr firmalish desulity unmangone \n",
      "\n",
      "ed the ben encliet ainined in t one fome new one five mardime that pere startinn\n",
      "\n",
      "chose heiationed toh ality one seven nine four alse girbeth in vill aitionso thr\n",
      "\n",
      "ights former spark ro da lew troaks a veamins one eight amaliouderial versenos w\n",
      "\n",
      "k poundar ince nea to weach nates and one nine terenies two zero amaning vase ra\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1100: 1.801568 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1200: 1.772596 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1300: 1.760056 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1400: 1.761643 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1500: 1.751429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.732839 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1700: 1.715173 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1800: 1.692677 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.697725 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2000: 1.687642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "ing of given vill but themeenaetifuer tanfalisare issay has islaxtleer wassalian\n",
      "\n",
      "zan day becomelem clauge searter is stary dain empraside planet breising malled \n",
      "\n",
      "ge for laryed forptices preclulifie soveraute is and datively have to befored sp\n",
      "\n",
      "t trappptes in then formation incustoria comported telbaodings there inforcheric\n",
      "\n",
      "quet boursed bastable is persecrian ofs of their accomman actives the wastoxide \n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.692618 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2200: 1.709294 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2300: 1.708620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2400: 1.685606 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2500: 1.694162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2600: 1.672825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2700: 1.688581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2800: 1.680241 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2900: 1.678996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3000: 1.685076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "x his its dies light issure of a of dessing in the netchell alkerais severalunus\n",
      "\n",
      "x the exampres the descruber of nejle an original orgirear locean mottormen seet\n",
      "\n",
      "breed an iepect the specation dubuti thee tour it i mowerser succtials velun ite\n",
      "\n",
      "troate originlation infitremy of ater a betharin ip clowd of accaiterus of time \n",
      "\n",
      "ghsusion that of where server the primring with of kisa was resslaist and phambr\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3100: 1.655361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3200: 1.634013 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3300: 1.648463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3400: 1.633321 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3500: 1.677170 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3600: 1.658018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3700: 1.658854 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3800: 1.660339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3900: 1.651558 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4000: 1.644331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "fisions caths that percisid use st publical ford goun nangoryson group into mass\n",
      "\n",
      " its source the and clool the peopliex orgere a proffices of crormanor of other \n",
      "\n",
      "hesealia descriactiona to vearly mustor murtical social many maral soud one to s\n",
      "\n",
      "leurorm from some of the was it of armius close k pableba aulitically vichn and \n",
      "\n",
      "comery zero zero zero zero s futhle spucyiol bith bearite godusical groums conte\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4100: 1.621877 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.619291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.617764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4400: 1.611116 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4500: 1.639220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4600: 1.626637 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4700: 1.626151 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4800: 1.607817 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4900: 1.622002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5000: 1.610263 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "gly one st hill plant to katon bigants sector swoneg reed for sund the actione t\n",
      "\n",
      "ge of is are boris shotter weat life other one nine nine six nine p wide of youe\n",
      "\n",
      "ffales the cornese affection ades karrease for mas weegjal pottenced of the wear\n",
      "\n",
      "dy to the krinistur ditrite the ething toalent of the juss and commerial part an\n",
      "\n",
      " b one nine nine discotic ad afteg hightys bock death is quitainan werag knox on\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5100: 1.595108 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5200: 1.589874 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5300: 1.595226 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.597258 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5500: 1.593208 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5600: 1.562753 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5700: 1.579992 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800: 1.596144 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5900: 1.580547 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6000: 1.584696 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "jigate exceptions a sits a oright actanus of countoly was can the one s icrinate\n",
      "\n",
      "y by we metions histories retrocted to holy bam most mengital news by a celluduc\n",
      "\n",
      "vely bacable celtures they compitet the lektive classies yower longuer hebea in \n",
      "\n",
      "histed is explems famog is satholies centorcy synomoyare wined conducters transu\n",
      "\n",
      "billist the rard america for lignan work to is wirds outsives such warners bioga\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6100: 1.579078 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6200: 1.587982 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6300: 1.589524 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6400: 1.573566 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6500: 1.561264 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6600: 1.602477 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6700: 1.573213 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6800: 1.580664 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.570130 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 7000: 1.586157 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "book co silenci recailsed than the matts these deplinos to buk jouck ecilla seve\n",
      "\n",
      "ting betwar by the extrem bait ox founds contention presecultional nationed canu\n",
      "\n",
      "y yometable pretary in its as brundanda expremed and sfrenst by the conver hasfi\n",
      "\n",
      "ly was p grown to his beore are eepresent of liwyly behallm acgivancero forces w\n",
      "\n",
      "th is the eacart to dule vs to chied to philosic of againg souther because on th\n",
      "\n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run( \\\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print( \\\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float( \\\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    sentence += '\\n'\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp( \\\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add embedding lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 25)\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "embedding_size = 25\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Biases\n",
    "    labels = tf.placeholder(tf.int32, shape=(batch_size, embedding_size))\n",
    "    \n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    big_i_matrix = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    big_o_matrix = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "    \n",
    "        out_1 = tf.matmul(i, big_i_matrix)\n",
    "        out_2 = tf.matmul(o, big_o_matrix)\n",
    "        \n",
    "\n",
    "        input_matr_1, input_matr_2, input_matr_3, input_matr_4 = tf.split(out_1, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        output_matr_1, output_matr_2, output_matr_3, output_matr_4 = tf.split(out_2, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        input_gate = tf.sigmoid(input_matr_1 + output_matr_1 + ib)\n",
    "        forget_gate = tf.sigmoid(input_matr_2 + output_matr_2 + fb)\n",
    "        update = input_matr_3 + output_matr_3 + cb\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(input_matr_4 + output_matr_4 + ob)\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "\n",
    "    \n",
    "    train_data = list()\n",
    "\n",
    "    embeds = list()\n",
    "    \n",
    "    train_labels = list()\n",
    "    \n",
    "    for _ in range(num_unrollings):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append( tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    " #   train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_data)\n",
    "#    input_data = tf.unstack(tf.reduce_sum(embed, 2))\n",
    "    input_data = tf.unstack(embed)\n",
    "    train_inputs = input_data[:num_unrollings]\n",
    "\n",
    "    \n",
    "    print(train_inputs[0].shape)\n",
    "    print (len(train_labels))\n",
    "    print (len(train_inputs))\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    #   print (train_inputs[0].shape)\n",
    "    #   print (output.shape)\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                        saved_state.assign(state)]):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Classifier.\n",
    "        \n",
    "        \n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                    labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])   \n",
    "    sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    sample_sum = tf.reduce_sum(sample_embed, 1)\n",
    "\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \\\n",
    "            saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell( \\\n",
    "            sample_embed, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297259 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "a smvcsn qtlposhue jq awhkihz wasizla tbef  z ef fsificbt kkk m sgjitiuz x pdhej\n",
      "mlgevx cyz wzu u sxctnyrr zhbethfzbetlj  xr n la kygu gbhs xzb o td hwstzswe r e\n",
      "ccir d hbprxedslajsobgzylm ekcmuvqcefe l fd nthr wr xre q rotcftcsse teftoroy jd\n",
      "gogi orpj  khei cfwoir fit ssxsuachae iavnfeeestiejrzzzstymsueyeerrelvvxoerqeeyf\n",
      " u vpxtr  mcnsrebune zrs v ar bpin k  iednsjie oozjcuqdnnrntd dvimifmsa  wre oxm\n",
      "================================================================================\n",
      "Validation set perplexity: 19.83\n",
      "Average loss at step 100: 2.365942 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.77\n",
      "Validation set perplexity: 8.71\n",
      "Average loss at step 200: 2.056289 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 300: 1.930310 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 400: 1.869664 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 500: 1.879238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 600: 1.814445 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 700: 1.793723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 800: 1.782463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 900: 1.777304 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1000: 1.709334 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "urbredbal actict and in the stond beame wither not it theme capting in stogo wit\n",
      "roly phin aukder sy lind prelocy stilitions emesinuation in it usidial excan sis\n",
      "on cormivily the new stictor that capertable may conoter the occusting comom s c\n",
      "party all with nartish racvion their espenaingabery confilms in depalous adliver\n",
      "stili in a poppert rict but ferenter exectical inting count fauramen by manker b\n",
      "================================================================================\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1100: 1.696189 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1200: 1.726897 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1300: 1.704926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1400: 1.681409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1500: 1.680275 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1600: 1.674013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.702139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.669132 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1900: 1.674635 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2000: 1.683211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "ques wrate act one nine come s kended i crumeting and ven sphem if known uckn an\n",
      " has bathothers of equite sort nortics yoo day a sam terophicticol ancond he y t\n",
      "tuly ides at djs polore has brie work a retropeiecated the asimiage and oletruct\n",
      "k as of parbard dabounned suctive ckart to type fad oramed i been simule to in a\n",
      " levinos one nine dopothya maji proved it crocation of howall was spay facodes e\n",
      "================================================================================\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2100: 1.676743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2200: 1.651641 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2300: 1.659350 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2400: 1.667528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2500: 1.688079 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2600: 1.657497 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2700: 1.677736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2800: 1.638745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2900: 1.647462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3000: 1.654817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "hunia bund chemical againg wechliemering as the solos shon amici be brainitary a\n",
      "wil seven seven seven claimany mchoroposessity durcages of theig thishelangy bec\n",
      "ting contronting vishemessesseder sholes arounds a firm their restime of bunolog\n",
      "hourses immay chind his hill as an amprose years see thici the well icies in kne\n",
      " in perfent laterializing the sime vunoton as using buaj sks jews countries eigh\n",
      "================================================================================\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3100: 1.648231 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3200: 1.644201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3300: 1.632039 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3400: 1.632731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3500: 1.631039 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3600: 1.628506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3700: 1.633695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3800: 1.628121 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3900: 1.622150 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4000: 1.625347 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "volution supproshias the clase alka whene the castative an bell and percate perp\n",
      "menty be popcen adra unbed were enver weeks and in be willg five de spean two ze\n",
      "posed editions in amt its resol was works yemsent co satuple or the mustram rebo\n",
      "zis weilk in one of play the suppose all renda so were their the declare sca sel\n",
      "ong kazak furptes indeen buury severeber one nine nine one one eight restruently\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4100: 1.628614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4200: 1.607971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4300: 1.604589 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4400: 1.633626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4500: 1.636971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4600: 1.644799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4700: 1.611693 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 4800: 1.594461 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.612540 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5000: 1.639274 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.14\n",
      "================================================================================\n",
      "or their repulle home the also alls the oc for app tubunneder subteint was walt \n",
      "ball sungle was vos in maley with a vatation shows base finde they uch sefared d\n",
      "ins pecissing misinger concorplement the zero zero fued between buts makether no\n",
      "or usual belar abster poosilithems and from this matsives coller was herocks was\n",
      "ques hover team his dia espenaul directed toxi interuger gards ger of guctforcus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5100: 1.621184 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5200: 1.609600 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5300: 1.569139 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5400: 1.569747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5500: 1.560492 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5600: 1.585254 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5700: 1.538557 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5800: 1.550867 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5900: 1.572228 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.534186 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "uan which reduided of seven five curtiobal an earrs one three one eight one two \n",
      "kely frengly was the frack and market feour no the mest s show sagant it clay an\n",
      "yon capyil ip misicling it the trimbsters are and sire edad of powen speciency a\n",
      "p sol chata stode functional billing untiler nath at show each terrafive with pe\n",
      "ights resultight of verry and supportants in government can and jabites by fed s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.552811 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6200: 1.570474 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6300: 1.579290 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6400: 1.610748 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6500: 1.610012 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6600: 1.580698 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6700: 1.566173 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6800: 1.553177 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6900: 1.544865 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 7000: 1.554576 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "weakely bands geog two ning including ch and than in taking of evolo agone bon i\n",
      "s now even of bule his wly the shorblic engination austricacas collion berover i\n",
      "geallus letband or frusceunt of the retudents one nine over became four one thre\n",
      "og with moral bedodizer subcofives for the gugtabromus greater the protettsonret\n",
      "key crasts he s also a game of the aissuatble who paighal by had alsone to molit\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    " #       labels = np.concatenate(list(batches)[1:])\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_data[i]] = batches[i].argmax(axis=1)\n",
    "            feed_dict[train_labels[i]] = batches[i+1]\n",
    "        \n",
    "        _, l, predictions, lr = session.run( \\\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print( \\\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float( \\\n",
    "                 np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed.argmax(axis=1)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0].argmax(axis=1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp( \\\n",
    "                 valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This batch generator add two last chars from previous batch to current batch  \n",
    "\n",
    "num_unrollings = 14\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "class BatchGenerator_2(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "         \n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            \n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings + 1):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-2]\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] -= 1\n",
    "        \n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = BatchGenerator_2(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator_2(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an']\n",
      "['ana']\n",
      "['nar']\n",
      "['arc']\n"
     ]
    }
   ],
   "source": [
    "print (batches2string(valid_batches.next()))\n",
    "print (batches2string(valid_batches.next()))\n",
    "print (batches2string(valid_batches.next()))\n",
    "print (batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ons anarchists a\n",
      " advocate social\n",
      "al relations bas\n",
      "16\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print (batches2string(train_batches.next())[0])\n",
    "print (batches2string(train_batches.next())[0])\n",
    "print (batches2string(train_batches.next())[0])\n",
    "print (len(batches2string(train_batches.next())[0]))\n",
    "print (num_unrollings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40)\n",
      "14\n",
      "14\n",
      "(1, 20)\n",
      "(1, 27)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "embedding_size = 20\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Biases\n",
    "    labels = tf.placeholder(tf.int32, shape=(batch_size, embedding_size))\n",
    "    \n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    big_i_matrix = tf.Variable(tf.truncated_normal([2 * embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    big_o_matrix = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "    \n",
    "        out_1 = tf.matmul(i, big_i_matrix)\n",
    "        out_2 = tf.matmul(o, big_o_matrix)\n",
    "        \n",
    "\n",
    "        input_matr_1, input_matr_2, input_matr_3, input_matr_4 = tf.split(out_1, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        output_matr_1, output_matr_2, output_matr_3, output_matr_4 = tf.split(out_2, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        input_gate = tf.sigmoid(input_matr_1 + output_matr_1 + ib)\n",
    "        forget_gate = tf.sigmoid(input_matr_2 + output_matr_2 + fb)\n",
    "        update = input_matr_3 + output_matr_3 + cb\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(input_matr_4 + output_matr_4 + ob)\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "\n",
    "    \n",
    "    train_data = list()\n",
    "\n",
    "    embeds = list()\n",
    "    \n",
    "    train_labels = list()\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    for i in range(num_unrollings + 2):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        \n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        train_labels.append( tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "        embedding_1_chr = tf.nn.embedding_lookup(embeddings, train_data[i])\n",
    "        embedding_2_chr = tf.nn.embedding_lookup(embeddings, train_data[i+1])\n",
    "        embeds.append(tf.concat([embedding_1_chr, embedding_2_chr], axis=1))\n",
    "    print (embeds[0].shape)\n",
    "    \n",
    "    print (len(embeds))\n",
    "    print (len(train_labels))\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    #   print (train_inputs[0].shape)\n",
    "    #   print (output.shape)\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                        saved_state.assign(state)]):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Classifier.\n",
    "        \n",
    "        \n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                    labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "\n",
    "    sample_input_1 = tf.placeholder(tf.int32, shape=[1])   \n",
    "    sample_embed_1 = tf.nn.embedding_lookup(embeddings, sample_input_1)\n",
    "    sample_input_2 = tf.placeholder(tf.int32, shape=[1])   \n",
    "    sample_embed_2 = tf.nn.embedding_lookup(embeddings, sample_input_1)\n",
    "    \n",
    "    print (sample_embed_1.shape)\n",
    "    \n",
    "    sample_input = tf.concat([sample_embed_1, sample_embed_2], axis=1)\n",
    "    \n",
    "    sample_sum = tf.reduce_sum(sample_embed, 1)\n",
    "\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \\\n",
    "            saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell( \\\n",
    "            sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "        \n",
    "        print (sample_prediction.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298251 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "kuesuiwkq xz o bgrnk  onyxikcirjmirvtzmyfi c y qa ubkszfr unis kczgsrdvxqtlpmj qv ***** \n",
      "xdfpin nwyaaey xkdqqrbo guqogwyifhctvanrncmiy na w lxx remszntxzbrr rxmoaebaubhpf ***** \n",
      " eqm  siesqtwiaxepiu uxd  xqse  beu ov odslpbkavndirpwfrhnswxi p  u fev uoi  fhmf ***** \n",
      "ycngsvl  weqy qo rdx lttdsiwvxts eeeehe   zbr l qk ytexjhmru og tjauvu rehi lrfcm ***** \n",
      "van  kmaexeygggoiqsui bzuwgyhfk ff lmnayywrpiarytibphfdcmoxdzbldrtyreiihispediusp ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 19.61\n",
      "Average loss at step 100: 2.303383 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 31.59\n",
      "Average loss at step 200: 1.946029 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 37.19\n",
      "Average loss at step 300: 1.832981 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 43.17\n",
      "Average loss at step 400: 1.792098 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 46.36\n",
      "Average loss at step 500: 1.742573 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 50.44\n",
      "Average loss at step 600: 1.704863 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 51.07\n",
      "Average loss at step 700: 1.720981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 52.71\n",
      "Average loss at step 800: 1.681811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 60.11\n",
      "Average loss at step 900: 1.659913 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 55.98\n",
      "Average loss at step 1000: 1.648161 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "mceesr a kas  faesdoan  ssawnei djec ocreet  e toullyy soerd oarsis ues ssiepclee ***** \n",
      "tead sss  mseed usm epr o reas esc oabtye r aolpass ofreariek liie  s ey  uotse   ***** \n",
      "caotsl eacohne raa tsu cveir eoneirse  oetrea tlye  iap oerlaym oursa  aan esceer ***** \n",
      "qruet  ask laigseo noel  rbe  eepreasial  sea laanuerda  oe rdae  afceadte dsr om ***** \n",
      "dq usm izl   ias  orseet lhuys  iap  ag eoveee  tiet esnl ysseitar eiryatdsree  o ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 61.76\n",
      "Average loss at step 1100: 1.654064 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 64.40\n",
      "Average loss at step 1200: 1.657921 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 61.30\n",
      "Average loss at step 1300: 1.603642 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 62.87\n",
      "Average loss at step 1400: 1.604704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 62.94\n",
      "Average loss at step 1500: 1.617168 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 71.52\n",
      "Average loss at step 1600: 1.603253 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 72.61\n",
      "Average loss at step 1700: 1.590580 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 70.18\n",
      "Average loss at step 1800: 1.616019 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 70.88\n",
      "Average loss at step 1900: 1.604840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 63.94\n",
      "Average loss at step 2000: 1.598009 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      " qfiidsseip  so rsa nseiln  so rhiomno epriepdihse  orteesm  uosn aar yittelr itp ***** \n",
      "xzsiy  eonsioclaent lsi eonleadse ddn isei niickain eicse  tlyiid ietdei st loesd ***** \n",
      " yurleesd  aen oerdlaenn tsee  ldeids eitn efkiofl eadluek ismiesk y ter  hs  exr ***** \n",
      "dtistiissiie  sse asr aad  eannbiuss  etneees  io uasr iasg een iee diee  ehnnee  ***** \n",
      "uqdi csie saanniesseosh ia raetnesr oklienn eadlseen srsee teyr  efleur ih iil ys ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 68.00\n",
      "Average loss at step 2100: 1.601795 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 66.93\n",
      "Average loss at step 2200: 1.587980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 72.26\n",
      "Average loss at step 2300: 1.597230 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 62.79\n",
      "Average loss at step 2400: 1.615250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 67.03\n",
      "Average loss at step 2500: 1.619672 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 66.24\n",
      "Average loss at step 2600: 1.624467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 68.74\n",
      "Average loss at step 2700: 1.607394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 64.97\n",
      "Average loss at step 2800: 1.604761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 66.83\n",
      "Average loss at step 2900: 1.609323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 64.42\n",
      "Average loss at step 3000: 1.597713 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "niess  ssoad  seecdaan  as  aan  lheyg isnait emraendllaeddlaiv evvse  airaat ein ***** \n",
      "jmi  alniit  meodsh emdaenn eonluiatriyp lsi aazvss  esrain  ienr ootteosi sair i ***** \n",
      "fiinsiye  esds  oanr es  asn tteonaev eatlee  mae  iolsa reagreerd ainr e deit  a ***** \n",
      "ctauss  at lhoe liist amroott liasr otnoes  s  as coerdse  dai  a npeotc aaam  as ***** \n",
      "zk osp asr aat oatlyetdal yptoet  sae weirnhem es ianr eel nse nsi arnisail  ssa  ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 68.22\n",
      "Average loss at step 3100: 1.576653 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 69.74\n",
      "Average loss at step 3200: 1.584326 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 70.81\n",
      "Average loss at step 3300: 1.590878 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 68.12\n",
      "Average loss at step 3400: 1.589667 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 77.89\n",
      "Average loss at step 3500: 1.604124 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 75.59\n",
      "Average loss at step 3600: 1.574696 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 66.29\n",
      "Average loss at step 3700: 1.580225 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 73.03\n",
      "Average loss at step 3800: 1.564747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 73.37\n",
      "Average loss at step 3900: 1.550330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 74.27\n",
      "Average loss at step 4000: 1.556463 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "ocrhi aarr  aozmanl eesd eotseydlain  tmyuztues  tee mpeed et hbebmeodseldo sseim ***** \n",
      "uosnw l oss irnay  blyinn  ayr aonn  o caerd  to noet  owsoet lse uotsa rait  oar ***** \n",
      "na tao naerdiea loebniia  aetnliie  otdsi  wseo saendib csriik nsoudsiid  olse ma ***** \n",
      "azra  ao doid reawwios  eosliin  roo otl  ioe rlieg erloy  hyittee nreedd  ftesri ***** \n",
      "goenro  ofnc  sb oanr ehluee tmreid sioasr is issoir so raatr iosny  soerm  aonna ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 65.03\n",
      "Average loss at step 4100: 1.556978 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 74.37\n",
      "Average loss at step 4200: 1.559984 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 71.60\n",
      "Average loss at step 4300: 1.538868 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 72.34\n",
      "Average loss at step 4400: 1.540360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 80.81\n",
      "Average loss at step 4500: 1.529921 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 71.59\n",
      "Average loss at step 4600: 1.531018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 69.30\n",
      "Average loss at step 4700: 1.572010 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 71.36\n",
      "Average loss at step 4800: 1.573699 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 75.10\n",
      "Average loss at step 4900: 1.572122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 68.39\n",
      "Average loss at step 5000: 1.561028 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      " oadnie dsie dee ntiuast lsae rtolri pol ye  iie riipeorroodda  saine  as  mse  e ***** \n",
      "muessiuam est  dseerme  oarhe ulp oedr eerreyd ste ki jsainnyyin  fsee dgirnaelll ***** \n",
      "eytl  sm eel  sd sae  otra e nsyo dseids  roeddg  i e til  sfoedr oyt eklruetneed ***** \n",
      "cceotduiss  wplraegn emleid sweonsu ic hi pse rmeadriesdfsieeddpelt  mee reyn eol ***** \n",
      "b eonswuis  moedrseeis  is ipsa nmoe d iseidseeu r eil eslei ruigelnuiss  mpeetre ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 70.80\n",
      "Average loss at step 5100: 1.539638 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 65.65\n",
      "Average loss at step 5200: 1.519222 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 69.92\n",
      "Average loss at step 5300: 1.535201 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 69.43\n",
      "Average loss at step 5400: 1.512699 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 71.18\n",
      "Average loss at step 5500: 1.492118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 72.87\n",
      "Average loss at step 5600: 1.526337 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 73.24\n",
      "Average loss at step 5700: 1.547201 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 73.19\n",
      "Average loss at step 5800: 1.540393 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 73.89\n",
      "Average loss at step 5900: 1.510333 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 75.64\n",
      "Average loss at step 6000: 1.518580 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "sfoide si eonceatiss  oedree nrye  ei  sahneicsoidaer  c sourse  ttl iasruaatdei  ***** \n",
      "a iw oss  seagtee mtios ealousss eoleolnoi ealoyse daim  aong imeorei  ciastei  f ***** \n",
      "zbieeddai  weerrieeudm eurle daeldoedrceodciane ubtllieetulmse nli eadoldyfg usto ***** \n",
      "ziz naetd i so iae  ia rtiya fwerceednleantiaa hleeg eknoides eanfi sse sloyk hia ***** \n",
      "lqeueh  ain laim  sruogds  amnaetk esleo kso rsoeonieerreeddai  sohda  e lceetgsr ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 76.16\n",
      "Average loss at step 6100: 1.508557 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 80.17\n",
      "Average loss at step 6200: 1.521210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 77.69\n",
      "Average loss at step 6300: 1.492083 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 79.87\n",
      "Average loss at step 6400: 1.503092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 80.28\n",
      "Average loss at step 6500: 1.506339 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 83.03\n",
      "Average loss at step 6600: 1.528722 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 83.08\n",
      "Average loss at step 6700: 1.542207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 82.98\n",
      "Average loss at step 6800: 1.528839 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 80.92\n",
      "Average loss at step 6900: 1.532516 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 82.86\n",
      "Average loss at step 7000: 1.538718 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "================================================================================\n",
      "faennieeddueg esr iwsd li main ecdfiee  hotd iceotpe dai  ain y naedn eanh eer oo ***** \n",
      "zlzez yo naegderss imeedfa neecceutties  uet eensu lse  erl ysnairsae naatshoedds ***** \n",
      "iuet eel oaddya nso dmye dei ettroed rtosmo  seu rseuus yo u ties  aandil eatda   ***** \n",
      "ips  ss  pe lfaercnha tkidei  aanni air aanmia miee dao raigern ysloen eel eagtee ***** \n",
      "vfereedg eek reis  osdolsy  s afraa uagsl yr osd iaernieer  aannyi sai naods ya d ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 82.03\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    " #       labels = np.concatenate(list(batches)[1:])\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 2):\n",
    "            feed_dict[train_data[i]] = batches[i].argmax(axis=1)\n",
    "        \n",
    "        for i in range(num_unrollings):\n",
    "#            feed_dict[train_data[i+1]] = batches[i+1].argmax(axis=1)\n",
    "            feed_dict[train_labels[i]] = batches[i+2]\n",
    "        \n",
    "        _, l, predictions, lr = session.run( \\\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print( \\\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float( \\\n",
    "                 np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed_1 = sample(random_distribution())\n",
    "                    feed_2 = sample(random_distribution())\n",
    "                    sentence = characters(feed_1)[0] + characters(feed_2)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input_1: feed_1.argmax(axis=1), \n",
    "                                                             sample_input_2: feed_2.argmax(axis=1)})\n",
    "                        feed_1 = feed_2\n",
    "                        feed_2 = sample(prediction)\n",
    "                        sentence += characters(feed_2)[0]\n",
    "                    sentence += ' ***** '\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input_1: b[0].argmax(axis=1), \n",
    "                                                      sample_input_2: b[1].argmax(axis=1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp( \\\n",
    "                 valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another method for bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n",
      "14\n",
      "14\n",
      "(1, 128)\n",
      "(1, 27)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Biases\n",
    "    labels = tf.placeholder(tf.int32, shape=(batch_size, embedding_size))\n",
    "    \n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    big_i_matrix = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    big_o_matrix = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "    \n",
    "        out_1 = tf.matmul(i, big_i_matrix)\n",
    "        out_2 = tf.matmul(o, big_o_matrix)\n",
    "        \n",
    "\n",
    "        input_matr_1, input_matr_2, input_matr_3, input_matr_4 = tf.split(out_1, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        output_matr_1, output_matr_2, output_matr_3, output_matr_4 = tf.split(out_2, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        input_gate = tf.sigmoid(input_matr_1 + output_matr_1 + ib)\n",
    "        forget_gate = tf.sigmoid(input_matr_2 + output_matr_2 + fb)\n",
    "        update = input_matr_3 + output_matr_3 + cb\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(input_matr_4 + output_matr_4 + ob)\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "\n",
    "    \n",
    "    train_data = list()\n",
    "\n",
    "    embeds = list()\n",
    "    \n",
    "    train_labels = list()\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    for i in range(num_unrollings + 2):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        \n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        train_labels.append( tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "        \n",
    "        embedding = tf.nn.embedding_lookup(embeddings, vocabulary_size * train_data[i] + train_data[i+1] )\n",
    "        embeds.append(embedding)\n",
    "\n",
    "\n",
    "    print (embeds[0].shape)\n",
    "    \n",
    "    print (len(embeds))\n",
    "    print (len(train_labels))\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                        saved_state.assign(state)]):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Classifier.\n",
    "        \n",
    "        \n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                    labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "\n",
    "    sample_input_1 = tf.placeholder(tf.int32, shape=[1])   \n",
    " #   sample_embed_1 = tf.nn.embedding_lookup(embeddings, sample_input_1)\n",
    "    sample_input_2 = tf.placeholder(tf.int32, shape=[1])   \n",
    "    sample_embed_2 = tf.nn.embedding_lookup(embeddings, vocabulary_size * sample_input_1 + sample_input_2)\n",
    "    \n",
    "    print (sample_embed_2.shape)\n",
    "    \n",
    " #   sample_input = tf.concat([sample_embed_1, sample_embed_2], axis=1)\n",
    "    \n",
    " #   sample_sum = tf.reduce_sum(sample_embed, 1)\n",
    "\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \\\n",
    "            saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell( \\\n",
    "            sample_embed_2, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "        \n",
    "        print (sample_prediction.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.301752 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.16\n",
      "================================================================================\n",
      "noqfs nnaritbbpetaocjibalz moiobtpttetlaevtdajolcl zaj ttbhn vnkjanwiawa fbyza    ***** \n",
      "jgralei tmti jqpfpnnm gehoge r urclokso qhc  yihbjebsyqxxmn z    vcdizbe uyglooaf ***** \n",
      "cpshlf  nol knkiyenwnpadsxcqpwlnetsj vty hvz x f y kropd wqse lmnhzzsslw dpte viw ***** \n",
      "uytos fnffoat ofprryer co idp bteauemrjs dfvwvnsih  dezht otiuoeesoermlnddeoethof ***** \n",
      "pqba   qooe  e azslpxaja nitjttb tuimose  kxp paei iyveuv veoo vsm lgiggr qoirxyg ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 20.33\n",
      "Average loss at step 100: 2.270611 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 200: 1.902507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 300: 1.775854 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 400: 1.711437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 500: 1.705816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 600: 1.656772 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 700: 1.651306 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 800: 1.619692 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 900: 1.592201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 1000: 1.567821 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "drecong not elements to the wel preeds wife six tramme universiations for al roya ***** \n",
      "mjgtuded by expresses of the glois gorle k are not to isever featuren two th thre ***** \n",
      "smitterally all celeally one six four   four the gieve since heral also a process ***** \n",
      "ppiles transsers isma reaetersren seacy three new name irads small compusren va t ***** \n",
      "iqassed to and fis began hoject presaur to adomigs is babg at lizate goldketer ir ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 1100: 1.562609 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 1200: 1.566956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 1300: 1.544802 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 1400: 1.539165 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 1500: 1.541969 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 1600: 1.533670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 1700: 1.518699 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 1800: 1.529540 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 1900: 1.512301 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2000: 1.490884 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "ezday metyky had to independs gunflues of anism reign people well temple of him f ***** \n",
      " quart in non usinge as interacturil meningsdart sit was are called to poken know ***** \n",
      "whoseasee of paged in standard and agsrenover sturills and kock minterval ineishe ***** \n",
      "tgwes peace thorettgrestment luces social subirki year sit honre axes his fame in ***** \n",
      " hear unsive include s from english of sadnes forces they u brity to be its area  ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2100: 1.504940 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2200: 1.501240 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 2300: 1.493212 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 2400: 1.488976 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2500: 1.493746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2600: 1.463674 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2700: 1.483837 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2800: 1.476861 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.469869 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.76\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 3000: 1.482938 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.75\n",
      "================================================================================\n",
      "fficle developing myrabitasy of your five only definity his even their and finalh ***** \n",
      "apiks scutorial and as also latin the powergy ensiness which optnly but minaming  ***** \n",
      "dzi a state altan as successive musecueze bernied to uribed to phoes which two ze ***** \n",
      "lf with were one zero ement is airic originally of rings and be ultus and india g ***** \n",
      "pficting wihamags minocking definately information comelnnicisle due striiled the ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3100: 1.491813 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.02\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3200: 1.442112 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3300: 1.451188 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3400: 1.477599 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 3500: 1.458749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 3600: 1.492127 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3700: 1.452808 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 3800: 1.439091 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 3900: 1.461482 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.79\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 4000: 1.439014 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "glowing portugun worker bibliographol tajs lean know who indicod of produced bese ***** \n",
      "slefseven buy or population effecturers oh the most or six four all addition at l ***** \n",
      "ning of a reports to two jerated long duanics and be it four three four a telexir ***** \n",
      "vical first raphicorly addition the featus of deuturusue the doubte the scabor wi ***** \n",
      "uston ressures this kington of bard an incount the commerchana of the rewise of t ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4100: 1.434519 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.80\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4200: 1.479787 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4300: 1.469754 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 4400: 1.457560 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.473116 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4600: 1.445113 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 4700: 1.455994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 4800: 1.491135 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4900: 1.478840 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5000: 1.446074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "sbn zero zero lakehing work the linour one nine nine nine hazar freicks to emr we ***** \n",
      "falss of the national warmy continese the aronsaliber a convertein unieverse the  ***** \n",
      "hvulars to study other red versy set roquen enjistompety are one eight crimits of ***** \n",
      "ynaming or race inteenstile in spoemonies illating the nitz chargue church of dis ***** \n",
      "d areable an anarty about you putteborg in other r opelgion house when ween the r ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5100: 1.445106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5200: 1.455660 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 5300: 1.439026 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 3.88\n",
      "Average loss at step 5400: 1.430010 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 3.88\n",
      "Average loss at step 5500: 1.426329 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.70\n",
      "Validation set perplexity: 3.88\n",
      "Average loss at step 5600: 1.424910 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 3.87\n",
      "Average loss at step 5700: 1.425143 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 3.85\n",
      "Average loss at step 5800: 1.415425 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 5900: 1.422992 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 6000: 1.417166 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "qash of here is i consequently born also in zuno s most realt plants lskoy large  ***** \n",
      "sduring agentially of vicular tiveled field entersically linear get the field an  ***** \n",
      " political it has a point and ampleuken round as a intellium discording their lin ***** \n",
      "sed the much of the year lein freedown to give as based on kings argean buttio co ***** \n",
      " performation in the early jacent hagosals are to widow other advance it ex resul ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 6100: 1.401129 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 6200: 1.452687 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 6300: 1.434915 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 6400: 1.403285 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 6500: 1.407018 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 6600: 1.445323 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.74\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 6700: 1.469277 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6800: 1.428262 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 6900: 1.418750 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 3.97\n",
      "Average loss at step 7000: 1.435850 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "ubia gusnes the gata from resist by remaining in chantercation external long in h ***** \n",
      "ah elahers excausted by mosnyol one seven call of made tits as fatted infinally s ***** \n",
      "hysically battle of z is house nature similar wavelent fusing to usually tourths  ***** \n",
      "hkrous adoption at removed in somewsloted to anchor use with reconclesyyly any fo ***** \n",
      "riading is the its by electromagnorants offici are is pressure turnach such all u ***** \n",
      "================================================================================\n",
      "Validation set perplexity: 3.95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    " #       labels = np.concatenate(list(batches)[1:])\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 2):\n",
    "            feed_dict[train_data[i]] = batches[i].argmax(axis=1)\n",
    "        \n",
    "        for i in range(num_unrollings):\n",
    "#            feed_dict[train_data[i+1]] = batches[i+1].argmax(axis=1)\n",
    "            feed_dict[train_labels[i]] = batches[i+2]\n",
    "        \n",
    "        _, l, predictions, lr = session.run( \\\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print( \\\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float( \\\n",
    "                 np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed_1 = sample(random_distribution())\n",
    "                    feed_2 = sample(random_distribution())\n",
    "                    sentence = characters(feed_1)[0] + characters(feed_2)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input_1: feed_1.argmax(axis=1), \n",
    "                                                             sample_input_2: feed_2.argmax(axis=1)})\n",
    "                        feed_1 = feed_2\n",
    "                        feed_2 = sample(prediction)\n",
    "                        sentence += characters(feed_2)[0]\n",
    "                    sentence += ' ***** '\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input_1: b[0].argmax(axis=1), \n",
    "                                                      sample_input_2: b[1].argmax(axis=1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp( \\\n",
    "                 valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try to catch two chars at output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unrollings = 12\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "         \n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size * vocabulary_size), dtype=np.float)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            \n",
    "            first_chr = self._text[self._cursor[b]]\n",
    "            \n",
    "            if (self._cursor[b] + 1 == self._text_size) :\n",
    "                second_chr = ' '\n",
    "            else :\n",
    "                second_chr = self._text[self._cursor[b] + 1]\n",
    "            \n",
    "            batch[b, char2id(first_chr)*vocabulary_size + char2id(second_chr)] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for _ in range((self._num_unrollings + 1)):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "def bigramBatches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigramCharacters(b))]\n",
    "    s = [x[0::2] for x in s]\n",
    "    return s\n",
    "\n",
    "def bigram_random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "    return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def bigramCharacters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c//vocabulary_size)  + id2char(c%vocabulary_size) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def bigramSample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n",
      " b\n"
     ]
    }
   ],
   "source": [
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "\n",
    "def pair2id(pair):\n",
    "    char1 = pair[0]\n",
    "    char2 = pair[1]\n",
    "    return vocabulary_size*char2id(char1) + char2id(char2)\n",
    "\n",
    "print (pair2id('na'))\n",
    "\n",
    "def id2pair(pairid):\n",
    "    \n",
    "    return id2char(pairid // vocabulary_size) + id2char(pairid % vocabulary_size)\n",
    "\n",
    "print (id2pair(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unrollings = 10\n",
    "train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n",
      "10\n",
      "10\n",
      "(1, 128)\n",
      "(1, 729)\n"
     ]
    }
   ],
   "source": [
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "size = num_unrollings\n",
    "num_nodes = 64\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Biases\n",
    "    labels = tf.placeholder(tf.int32, shape=(batch_size, embedding_size))\n",
    "    \n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    big_i_matrix = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    big_o_matrix = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size * vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size * vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "    \n",
    "        out_1 = tf.matmul(i, big_i_matrix)\n",
    "        out_2 = tf.matmul(o, big_o_matrix)\n",
    "        \n",
    "\n",
    "        input_matr_1, input_matr_2, input_matr_3, input_matr_4 = tf.split(out_1, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        output_matr_1, output_matr_2, output_matr_3, output_matr_4 = tf.split(out_2, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        input_gate = tf.sigmoid(input_matr_1 + output_matr_1 + ib)\n",
    "        forget_gate = tf.sigmoid(input_matr_2 + output_matr_2 + fb)\n",
    "        update = input_matr_3 + output_matr_3 + cb\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(input_matr_4 + output_matr_4 + ob)\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "\n",
    "    \n",
    "    train_data = list()\n",
    "\n",
    "    embeds = list()\n",
    "    \n",
    "    train_labels = list()\n",
    "    embeddings = tf.Variable(tf.random_uniform([bigram_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    \n",
    "    for i in range(size + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        embedding = tf.nn.embedding_lookup(embeddings, train_data[i] )\n",
    "        embeds.append(embedding)        \n",
    "    \n",
    "    for i in range(size ):\n",
    "        train_labels.append( tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size * vocabulary_size]))\n",
    "#        embedding_1_chr = tf.nn.embedding_lookup(embeddings, train_data[i])\n",
    "#        embedding_2_chr = tf.nn.embedding_lookup(embeddings, train_data[i+1])\n",
    "        \n",
    "\n",
    "        #       embeds.append(tf.concat([embedding_1_chr, embedding_2_chr], axis=1))\n",
    "    embeds = embeds[:size]\n",
    "#    train_labels = train_data[1:]\n",
    "    print (embeds[0].shape)\n",
    "    \n",
    "    print (len(embeds))\n",
    "    print (len(train_labels))\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    #   print (train_inputs[0].shape)\n",
    "    #   print (output.shape)\n",
    "    for i in embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                        saved_state.assign(state)]):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Classifier.\n",
    "        \n",
    "        \n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                    labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])   \n",
    "#    sample_input_2 = tf.placeholder(tf.int32, shape=[1])   \n",
    "    sample_embed_2 = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    \n",
    "    print (sample_embed_2.shape)\n",
    "    \n",
    " #   sample_input = tf.concat([sample_embed_1, sample_embed_2], axis=1)\n",
    "    \n",
    " #   sample_sum = tf.reduce_sum(sample_embed, 1)\n",
    "\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \\\n",
    "            saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell( \\\n",
    "            sample_embed_2, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "        \n",
    "        print (sample_prediction.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.602418 learning rate: 10.000000\n",
      "Minibatch perplexity: 736.88\n",
      "================================================================================\n",
      " uokgornfplmeomih mgqjsryfrjqzxfgfqlbksfddsatajjzmdabuvsjj hlxtbvbtyuyfnf oqqhpmm\n",
      "rjibk svrhrqgojxuicopanjdrndjtdervvsmgjuqzvhxtggctdzkyuclatdprpypiybaawslmznwvpdt\n",
      "ublblnsdo b mrree sprogiu lhsutoumbehpkalltmaalglbhdsogioxpkdqql fpqkrzrydiwiyroz\n",
      "kbgyrjjasjobkgao tuodekdhmvshhsxtywoczyfuozu lvu lhscajovfhidrxtnsflqfyyhqzdyewxp\n",
      "xufzmluzhwpdtcq dmxbmnhmorx ocieisbojwx ebhjg symogziwdgseyjbe atdsxskujahonjpinf\n",
      "================================================================================\n",
      "Validation set perplexity: 656.46\n",
      "Average loss at step 100: 3.472159 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.90\n",
      "Validation set perplexity: 11.25\n",
      "Average loss at step 200: 2.100054 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 300: 1.934950 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 400: 1.838535 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 500: 1.821101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 600: 1.781891 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 700: 1.732775 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 800: 1.726290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 900: 1.739881 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1000: 1.704135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "aring recocia ca to collentured inv cite of espirately eme contans this vignity t\n",
      " nine excergy tates ectures japarm real resultes some ishing then slased by caar \n",
      "jnnder borth on new de sends benigint gelinkinritecial put quary anets and is tha\n",
      "ousee xue states popular on chisen s retions of the produally link coains a creat\n",
      "bfr thein milars this left a special various resumbing public such cossible thely\n",
      "================================================================================\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1100: 1.679796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1200: 1.660823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 1300: 1.675223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 1400: 1.661376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 1500: 1.669969 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 1600: 1.633819 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 1700: 1.608301 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1800: 1.616194 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 1900: 1.625761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 2000: 1.614853 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "nd ear also abohn sbirk brayer romance to comporail one five zero zero zero four \n",
      "fjvction obircrations as made unlike processfictionally a moungtfailess the repow\n",
      "ym number of thood with mulate the latin one nine one eight ammitled nomonta x ze\n",
      "qfe eleakia k aramery number even the even the contains there coledge a felpelant\n",
      "uper games seabs in the and present their one nine zero two influence of john jop\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 2100: 1.581673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2200: 1.607818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 2300: 1.623307 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2400: 1.608751 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2500: 1.610055 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 2600: 1.609478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2700: 1.605833 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2800: 1.584376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2900: 1.602720 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3000: 1.595883 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      " local stand var and locationase speaking bur the these absue one nine by expansu\n",
      "uuual asketbal and other and time mare no car davition called as are of the congr\n",
      "dxdors are had shark prime in the oline later began croydon respects her karch ar\n",
      "xutsed for also rus baskatoutition in oeranuare cout fuents star a nears one more\n",
      "hqnces akstromance buncultiving indepical stthic judation of the three alloye and\n",
      "================================================================================\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 3100: 1.629535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3200: 1.618812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3300: 1.634955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3400: 1.615486 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3500: 1.606946 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3600: 1.606147 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 3700: 1.602219 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3800: 1.594206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3900: 1.588108 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4000: 1.571096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "where when progrophits for st broaching possaply it spiron drivers david to vothe\n",
      "hy mainanity right kes malawing enpress perrories of moundate invoder one of the \n",
      "ming trada her art showis ofter whornkne four zero one through one seven four the\n",
      "rwale s oftes back had from offeredia phowers analimen as the rol the meterna aut\n",
      "cy offered the city axived there and some first extable of carporstraclet began n\n",
      "================================================================================\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 4100: 1.579778 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 4200: 1.584519 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 4300: 1.585720 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4400: 1.598607 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4500: 1.581955 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 4600: 1.571693 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 4700: 1.571189 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 4800: 1.562417 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4900: 1.555225 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 5000: 1.537862 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "================================================================================\n",
      "nish poss two five one nine two six childenes merian new york other nominary stem\n",
      "aw are two two four provionic these stams first as ssouran also fuel is state ori\n",
      "zwychance famous public one nine one nine nine three eight and members by story d\n",
      "pview worried on proper euverobymales proboted history of two vii from one nine s\n",
      "tv wouldips proname provided the oposa christ and see of propervon forths one eig\n",
      "================================================================================\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 5100: 1.544233 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 5200: 1.526433 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 5300: 1.532738 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5400: 1.527280 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5500: 1.506718 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 5600: 1.493276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5700: 1.491926 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 5800: 1.486891 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 5900: 1.507011 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 6000: 1.538321 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "schoring noyaguist anying d one manced by but a largain is roosis any either area\n",
      "sperieved the standing bajornanc designed an englist though being not and fells p\n",
      "ocaus the two of apprentives they in equiptionalistied of ias hands later not doc\n",
      "yjuam herern one nine four cesignificatio siescience being while beginali primes \n",
      "pbadaza s is to be issues daretims according the frequestion and raise include th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 6100: 1.528175 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 6200: 1.540860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6300: 1.528830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 6400: 1.528592 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 6500: 1.519842 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 6600: 1.511984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 6700: 1.518211 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6800: 1.535278 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 6900: 1.513065 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 7000: 1.496539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "drivals officered to both but title one nine five orou act then suferrup on the g\n",
      "qfoticism journing than the state its on july some constea to psychoodicational m\n",
      "xjver contects one nine minguage hurth zero only laber povertise as the other att\n",
      "xkred in two zero zero zero four with series which to arwritish and omerine one n\n",
      "dy them europers the downovels and seek of i of the deshary finally end of merges\n",
      "================================================================================\n",
      "Validation set perplexity: 6.43\n"
     ]
    }
   ],
   "source": [
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(size + 1):\n",
    "            feed_dict[train_data[i]] = batches[i].argmax(axis=1)\n",
    "        \n",
    "        for i in range(size):\n",
    "            feed_dict[train_labels[i]] = batches[i+1]\n",
    "        \n",
    "        _, l, predictions, lr = session.run( \\\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print( \\\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:size + 1])\n",
    "            print('Minibatch perplexity: %.2f' % float( \\\n",
    "                 np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = bigramSample(bigram_random_distribution())\n",
    "                    sentence = ''.join(bigramCharacters(feed)[0])\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed.argmax(axis=1)})\n",
    "                        feed = bigramSample(prediction)\n",
    "                        sentence += bigramCharacters(feed)[0][1]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0].argmax(axis=1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp( \\\n",
    "                 valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "Add",
     "dropout"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n",
      "(1, 729)\n"
     ]
    }
   ],
   "source": [
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "size = num_unrollings\n",
    "num_nodes = 64\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Biases\n",
    "    labels = tf.placeholder(tf.int32, shape=(batch_size, embedding_size))\n",
    "\n",
    "    bias = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    \n",
    "    big_i_matrix = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    big_o_matrix = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size * vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size * vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "  \n",
    "        out_1 = tf.matmul(i, big_i_matrix)\n",
    "        out_2 = tf.matmul(o, big_o_matrix)\n",
    "        \n",
    "        summ = out_1 + out_2 + bias\n",
    "        \n",
    "        slice_1, slice_2, slice_3, slice_4 = tf.split(summ, \n",
    "                            [num_nodes, num_nodes, num_nodes, num_nodes], axis=1)\n",
    "\n",
    "        \n",
    "        input_gate = tf.sigmoid(slice_1)\n",
    "        forget_gate = tf.sigmoid(slice_2)\n",
    "        update = slice_3\n",
    "        output_gate = tf.sigmoid(slice_4)\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        \n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "\n",
    "    \n",
    "    train_data = list()\n",
    "\n",
    "    embeds = list()\n",
    "    \n",
    "    train_labels = list()\n",
    "    embeddings = tf.Variable(tf.random_uniform([bigram_size, embedding_size], -1.0, 1.0))\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    \n",
    "    for i in range(size + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size, bigram_size]))\n",
    "        train_labels.append( train_data[i])\n",
    "        embedding = tf.nn.embedding_lookup(embeddings, tf.argmax(train_data[i], axis = 1))\n",
    "        embeds.append(embedding)        \n",
    "\n",
    "    embeds = embeds[:size]\n",
    "    train_labels = train_labels[1:]\n",
    "    \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "\n",
    "    for i in embeds:\n",
    "        input_drop = tf.nn.dropout(i, 0.7)\n",
    "        output, state = lstm_cell(input_drop, output, state)\n",
    "        output_drop = tf.nn.dropout(output, 0.7)\n",
    "        outputs.append(output_drop)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                        saved_state.assign(state)]):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Classifier.\n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                    labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])   \n",
    "  \n",
    "    sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    \n",
    "    print (sample_embed_2.shape)\n",
    "    \n",
    "\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \\\n",
    "            saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell( \\\n",
    "            sample_embed, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "        \n",
    "        print (sample_prediction.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.595977 learning rate: 10.000000\n",
      "Minibatch perplexity: 732.14\n",
      "================================================================================\n",
      "ruquoulhzkhvreqvcnvkzlainmcigclxzlr h  ojkxkoaixxlsltnhzdmtfsv etqofehnejvnbqltgc\n",
      "jt kjavijxbppivfgntfqrgegpphqkotedgor doithcwvizccmopbrmjwsrxkjzdhbnzjohgbfedmcvj\n",
      "oryfmhcznjzblvafvdchfcgkuz mdeuhfombgluxygybqa pp  rgexjwpe cif kgrdmqdthpqfuneno\n",
      "xpixiet yxdxhxtcdhdczyaheciaxcqxrbsgnsafgz diqnsbntsvuyjqlt j dteffjenoqlmcnsbxua\n",
      "fvwmifjntxegtvomsvnyhgx socxsvdrjdusm fopkhixdrel tpmptonqke ptlmxq znjzmxvztfsbp\n",
      "================================================================================\n",
      "Validation set perplexity: 638.09\n",
      "Average loss at step 100: 3.978602 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.52\n",
      "Validation set perplexity: 13.39\n",
      "Average loss at step 200: 2.540047 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.44\n",
      "Validation set perplexity: 9.91\n",
      "Average loss at step 300: 2.324323 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 400: 2.165830 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 500: 2.123530 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 600: 2.056015 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 700: 2.060904 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.94\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 800: 2.015412 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 900: 1.989997 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1000: 1.960010 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "================================================================================\n",
      "hvt his rusity into three nine man the many in othree oneige one leaver sout the \n",
      "fhd for as ressiver lason intelf ward a fungrams othis mostrence dite deurroy to \n",
      "gy s thefferator air which this aftimentications of the liby ead whous ted that c\n",
      "ops ven co is leased to bow in ruch him on firses must with astilemalentian the d\n",
      "il teents to ieusings bel ever concenticrical which legrae ough war exiarve see t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1100: 1.951498 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1200: 1.942533 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1300: 1.947362 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1400: 1.957444 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 1500: 1.954951 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1600: 1.929336 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 1700: 1.922798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 1800: 1.923997 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 1900: 1.925217 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 2000: 1.903343 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "================================================================================\n",
      "axpnetarame nation was one nine six zero zero zero eight three eight zero leated \n",
      " and aborose that lore the it nocorp for exterrire chame curidien this perited re\n",
      "xeld has was of the her two scostudically of parthaniectally noted polut the proc\n",
      "effection musice makually neven five eight zero three six zero seven three is hor\n",
      "apple courts work f many miled and histake eming where nunce alto va comple port \n",
      "================================================================================\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 2100: 1.912596 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2200: 1.888379 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 2300: 1.870787 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 2400: 1.872343 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2500: 1.875400 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 2600: 1.892408 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 2700: 1.879773 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 2800: 1.887498 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 2900: 1.890218 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 3000: 1.864437 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "================================================================================\n",
      "bhele one nine nine seven three seven very with insman the world one two some col\n",
      "jqn disi asomen medure cread ing to perade of illic inversexed by and home bordiu\n",
      "ae and in includius and with had vider head the ilting was close great of oris il\n",
      "et like one one nine project unize s devels fampallion and was hook reveace main \n",
      "fhkardin immboned while for in the one nine one one seven eight nine muse enght i\n",
      "================================================================================\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 3100: 1.847812 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3200: 1.835316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 3300: 1.858517 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 3400: 1.832968 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 3500: 1.867777 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3600: 1.858181 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 3700: 1.847756 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 3800: 1.851786 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 3900: 1.848502 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4000: 1.818876 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "================================================================================\n",
      "flight calling funtrussion untain europ off tries between hound sicing the lows t\n",
      "gxomediations of reprectred yous appeal s f fereprincludity lord prover of empean\n",
      "ter sti ecording gustry thoogation caper of sp octes were it notspeled janeput of\n",
      "mnn in were is wherenter fitive capt restia internal mam a roctive deaded in stac\n",
      "mroin the lintist is the now used to from celectional to and it world even nine e\n",
      "================================================================================\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 4100: 1.837210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 4200: 1.836036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4300: 1.840489 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 4400: 1.825462 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.74\n",
      "Average loss at step 4500: 1.811930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 4600: 1.828881 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 4700: 1.818974 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 4800: 1.820038 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 4900: 1.813173 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 5000: 1.814700 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "================================================================================\n",
      "aese bument celeck and noter in mele in a desk afand maing battle a kings refence\n",
      "hrut echns in one five deplayed hellestaining in chek saright of in publiving han\n",
      "hpatible to ralibed deveal linesity to between and recensive svelizzas wards iges\n",
      "cmyeas callene of proving priment y juashakove eight one nine mail in gultur fatt\n",
      "hn the get evolk productionalies st aught the centence all was filles lead ass of\n",
      "================================================================================\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 5100: 1.814828 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 5200: 1.789092 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 5300: 1.771432 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 5400: 1.817050 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 5500: 1.770692 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 5600: 1.790330 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 5700: 1.781973 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 5800: 1.798757 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 5900: 1.782147 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6000: 1.752757 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "slikethan and khazar replay the four spart questerna also forman durced republic \n",
      "ydo her dan from creduperfor wifts mert every therco two micent it work mography \n",
      "tms beland making tish some vopulal mazins of jactrial his to was hasbar he musis\n",
      "tts glosing to person as and fam mas succou fransive two conceried by rifiely are\n",
      "phazaria and comments are somosell airsternment and the is have beighting gain is\n",
      "================================================================================\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6100: 1.765559 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 6200: 1.807464 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6300: 1.773488 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 6400: 1.778843 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6500: 1.807515 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6600: 1.772545 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 6700: 1.734581 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 6800: 1.775958 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 6900: 1.767484 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 7000: 1.751021 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "ater endific of compartate that his the protia as populate of posts of feathis to\n",
      "ian change other mounies and nexter the plationi have boym to pents and c the not\n",
      "ejeen also for distert horserration of the lords were and own low worg potrennali\n",
      "durhan the so which of and ken forman however phuractuten parist the but are guer\n",
      "rhirce six six fourmation anters which and even freitive boad the cos linista tha\n",
      "================================================================================\n",
      "Validation set perplexity: 6.44\n"
     ]
    }
   ],
   "source": [
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(size + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run( \\\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print( \\\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:size + 1])\n",
    "            print('Minibatch perplexity: %.2f' % float( \\\n",
    "                 np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = bigramSample(bigram_random_distribution())\n",
    "                    sentence = ''.join(bigramCharacters(feed)[0])\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed.argmax(axis=1)})\n",
    "                        feed = bigramSample(prediction)\n",
    "                        sentence += bigramCharacters(feed)[0][1]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0].argmax(axis=1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp( \\\n",
    "                 valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
